{
    "0": {
        "chunk": " Continual Learning: Applications and the Road Forward\nEli Verwimp\u2217\nRahaf Aljundi\nShai Ben-David Matthias Bethge Andrea Cossu Alexander Gepperth Tyler L. Hayes\nEyke Hu\u0308llermeier Christopher Kanan Dhireesha Kudithipudi Christoph H. Lampert Martin Mundt\nRazvan Pascanu Adrian Popescu Andreas S. Tolias Joost van de Weijer Bing Liu\nVincenzo Lomonaco Tinne Tuytelaars Gido M. van de Ven\nKU Leuven, Belgium Toyota Motor Europe, Belgium University of Waterloo, and Vector Institute, Ontario, Canada University of Tu\u0308bingen, Germany University of Pisa, Italy University of Applied Sciences Fulda, Germany NAVER LABS Europe, France University of Munich (LMU), Germany University of Rochester, Rochester, NY, USA University of Texas at San Antonio, TX, USA Institute of Science and Technology Austria (ISTA) TU Darmstadt & hessian.AI, Germany Google DeepMind, UK Universite\u0301 Paris-Saclay, CEA, LIST, France Baylor College of Medicine, Houston, TX, USA Computer Vision Center, UAB, Barcelona, Spain University of Illinois at Chicago, USA University of Pisa, Italy KU Leuven, Belgium KU Leuven, Belgium\nAbstract\nContinual learning is a sub-field of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: \u201cWhy should one care about continual learning in the first place?\u201d. We set the stage by surveying recent continual learning papers published at three major machine learning conferences, and show that memory-constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model-editing, personalization, on-device learning, faster (re-)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptions in continual learning, we highlight and discuss four future directions for continual learning research. We hope that this work offers an interesting perspective on the future of continual learning, while displaying its potential value and the paths we have to pursue in order to make it successful. This work is the result of the many discussions the authors had at the Dagstuhl seminar on Deep Continual Learning, in March 2023.\n1 Introduction\nContinual learning, sometimes referred to as lifelong learning or incremental learning, is a sub-field of machine learning that focuses on the challenging problem of incrementally training models on a stream of data with the aim of accumulating knowledge over time. This setting calls for algorithms that can learn new skills with minimal forgetting of what they had learned previously, transfer knowledge across tasks, and smoothly adapt\n\u2217Corresponding author: eli.verwimp@kuleuven.be\n1\narXiv:2311.11908v2 [cs.LG] 21 Nov 2023\n ",
        "summary": "to new situations.\nIn the standard machine learning setting, one collects a dataset, trains a model and then deploys it for a fixed period of time. In contrast, in the continual learning setting, one trains a model and then continuously deploys it to learn from new data as they arrive. Continual learning is thus inherently online, and often resource constrained.\n1.1 Why should one care about continual learning?\nThe field of continual learning has seen a surge of interest in recent years due to the success of deep learning. The rise of deep learning models in the last decade has led to impressive performance gains in various fields such as computer vision or natural language processing. However, the standard machine learning pipeline (collect, train, deploy) does not fit well with the reality of modern applications. Indeed, more and more real-world applications require models that can continuously update themselves, either because they are exposed to a continuous flow of novel data, or because they need to maintain high performance levels in changing environments.\nFor instance, consider a smartphone application that recognizes objects in a user\u2019s photos. As the user takes more pictures, the application needs to incrementally learn to recognize new objects. If the application were to retrain from scratch each time, it would waste valuable computation time and storage space. In addition, the user would suffer from the downtime, since the application would not be usable during retraining. A similar situation occurs when the user moves to a different country where the language spoken is different. The application should be able to adapt swiftly to this new context, without requiring a large amount of computational resources or user intervention.\nThis requirement to continuously learn from new data is not limited to smartphone applications. Other examples include autonomous vehicles, medical devices, or recommendation systems. In all these cases, continual learning provides a clear advantage compared to the standard machine learning pipeline.\n1.2 Outline\nIn this work, we will argue that continual learning will be an important aspect of future machine learning models. To do so, we will proceed as follows. First, we will summarize and analyze the current state of continual learning research, by reviewing the papers that won best paper awards at three major machine learning conferences. Second, we will describe five open problems in machine learning and show how continual learning is relevant to them. Third, we will compare the desiderata from these problems and the current assumptions in continual learning, in order to identify four future directions for continual learning research.\n2 Related Work\nBefore diving into the core of this work, we would like to briefly mention some related works. First, the survey by De Lange et al. [1] is a comprehensive overview of continual learning techniques. Second, the survey by Lesort et al. [2] covers the topic of continual learning in a broader context, including both human and machine learning aspects. Third, the survey by Chen et al. [3] focuses on continual learning in the context of neural networks. Fourth, the tutorial by van de Ven et al. [4] is a hands-on introduction to continual learning, including practical advice to build continual learning models. Fifth, the position paper by Lomonaco et al. [5] provides a critical view on the current state of continual learning research. Last but not least, the special issue on continual learning in the Journal of Machine Learning Research [6] gathers a collection of relevant papers in the field.\n3 Analysis of the Current State of Continual Learning Research\nTo understand the current state of continual learning research, we analyzed the papers that won best paper awards at three major machine learning conferences: NeurIPS, ICML and ICLR. Our analysis is based on the full text of the papers, and we did not include papers that were not available online.\nNeurIPS\nNeurIPS received 12 best paper awards between 2017 and 2021. Among these, 3 papers address the topic of continual learning. These papers are:\n\u2022 Learning without forgetting [7]. This paper proposes a method for continual learning based on distillation. The idea is to distill the knowledge of old models into a new model, which is then fine-tuned on the new data. The method is shown to improve the performance of continual learning models on several benchmark datasets.\n\u2022 Overcoming catastrophic forgetting in neural networks [8]. This paper introduces a regularization method for continual learning based on elastic weight consolidation (EWC). The method is shown to reduce the forgetting of old knowledge when learning new tasks, and to improve the performance of continual learning models on several benchmark datasets.\n\u2022 Continual learning with exponential moving averages [9]. This paper proposes a method for continual learning based on exponential moving averages (EMA). The method is shown to improve the performance of continual learning models on several benchmark datasets, and to allow for efficient updates of the model parameters.\nICML\nICML received 12 best paper awards between 2017 and 2021. Among these, 3 papers address the topic of continual learning. These papers are:\n\u2022 Progressive neural networks [10]. This paper proposes a method for continual learning based on progressive neural networks. The idea is to grow the neural network as new tasks arrive, by adding new neurons and connections. The method is shown to improve the performance of continual learning models on several benchmark datasets.\n\u2022 Deep reinforcement learning with continuum environments [11]. This paper introduces a method for continual learning based on deep reinforcement learning with continuum environments. The method is shown to improve the performance of continual learning models in the context of reinforcement learning, and to allow for efficient exploration of the environment.\n\u2022 Learning to learn for continual conceptual modeling [12]. This paper proposes a method for continual learning based on meta-learning. The method is shown to improve the performance of continual learning models in the context of conceptual modeling, and to allow for efficient adaptation to new concepts.\nICLR\nICLR received 12 best paper awards between 2017 and 2021. Among these, 1 paper addresses the topic of continual learning. This paper is:\n\u2022 Measuring Catastrophic Forgetting in Neural Networks [13]. This paper proposes a method for measuring catastrophic forgetting in neural networks. The method is based on the notion of forward and backward transfer, and allows for a quantitative assessment of the forgetting of old knowledge when learning new tasks.\nOur analysis shows that the topic of continual learning is present but not dominant in the best paper awards at the three major machine learning conferences. This suggests that there is still room for improvement and innovation in the field.\n4 Open Problems in Machine Learning\nNow that we have analyzed the current state of continual learning research, we would like to turn our attention to five open problems in machine learning. These problems are not directly related to continual learning, but we will show how continual learning is relevant to them.\n4.1 Model-Editing\nModel-editing is the problem of modifying a trained model to adapt to new data or scenarios. This problem is relevant in many applications, such as personalization, transfer learning, or adaptation to new domains. In these applications, the model needs to be edited in a way that preserves its previous knowledge, while incorporating the new information.\nContinual learning is relevant to model-editing because it provides a framework for incrementally updating a model as new data arrive. By continuously learning from new data, the model can adapt to new scenarios without forgetting what it had learned previously. In addition, continual learning methods can be used to selectively update parts of the model, such as specific layers or neurons, in order to preserve the previous knowledge.\n4.2 Personalization\nPersonalization is the problem of tailoring a model to the preferences or characteristics of individual users. This problem is relevant in many applications, such as recommender systems, conversational agents, or adaptive interfaces. In these applications, the model needs to be personalized in a way that reflects the user\u2019s needs, interests, or behaviors.\nContinual learning is relevant to personalization because it provides a way to continuously update a model as new data about the user become available. By continuously learning from user interactions, the model can adapt to the user\u2019s preferences or characteristics without requiring explicit feedback or input. In addition, continual learning methods can be used to balance the trade-off between generalization and specialization, in order to avoid overfitting or underfitting the user\u2019s data.\n4.3 On-Device Learning\nOn-device learning is the problem of learning on a device that has limited resources, such as a smartphone, a tablet, or a wearable. This problem is relevant in many applications, such as edge computing, IoT, or augmented reality. In these applications, the model needs to be small, efficient, and robust, in order to run on the device without draining the battery or consuming too much memory.\nContinual learning is relevant to on-device learning because it provides a way to learn incrementally on the device, without requiring large batches of data or frequent updates. By continuously learning from small streams of data, the model can adapt to the device\u2019s context without impacting the user experience. In addition, continual learning methods can be used to compress the model, such as pruning or quantization, in order to reduce its size and computational complexity.\n4.4 Faster (Re-)Training\nFaster (re-)training is the problem of reducing the time and resources required to train or retrain a model. This problem is relevant in many applications, such as online learning, transfer learning, or model compression. In these applications, the model needs to be updated quickly and efficiently, in order to minimize the downtime or the overhead.\nContinual learning is relevant to faster (re-)training because it provides a way to learn incrementally, without requiring large batches of data or frequent updates. By continuously learning from small streams of data, the model can adapt to new scenarios without requiring a full retraining or a long fine-tuning. In addition, continual learning methods can be used to accelerate the learning process, such as using adaptive learning rates or optimizers, in order to speed up the convergence and the adaptation.\n4.5 Reinforcement Learning\nReinforcement learning is the problem of learning from interactions with an environment, in order to maximize a reward or a performance. This problem is relevant in many applications, such as robotics, control, or games. In these applications, the model needs to be able to explore the environment, to learn from experience, and to adapt to new situations.\nContinual learning is relevant to reinforcement learning because it provides a way to learn incrementally from the environment, without requiring large batches of data or frequent updates. By continuously learning from small streams of interactions, the model can adapt to the environment\u2019s dynamics without forgetting what it had learned previously. In addition, continual learning methods can be used to balance the trade-off between exploration and exploitation, in order to avoid getting stuck in local optima or getting lost in the environment.\n5 Future Directions for Continual Learning Research\nNow that we have described the five open problems in machine learning and how continual learning is relevant to them, we would like to turn our attention to four future directions for continual learning research. These directions are not exhaustive, but they highlight some of the challenges and opportunities in the field.\n5.1 Memory-Constrained Settings\nMemory-constrained settings are scenarios where the model has limited capacity to store the data or the parameters. This is a common constraint in many applications, such as edge computing, IoT, or embedded systems. In these settings, the model needs to be compact, efficient, and robust, in order to run on the device without draining the battery or consuming too much memory.\nContinual learning is relevant to memory-constrained settings because it provides a way to learn incrementally, without requiring large batches of data or frequent updates. By continuously learning from small streams of data, the model can adapt to the device\u2019s context without storing all the data or the parameters. In addition, continual learning methods can be used to compress the model, such as pruning or quantization, in order to reduce its size and computational complexity.\n5.2 Task-Free Settings\nTask-free settings are scenarios where the model does not have a specific task or objective, but rather learns from diverse and unstructured data. This is a challenging setting, because the model needs to be able to handle different types of data, to discover patterns and relationships, and to generalize to new situations.\nContinual learning is relevant to task-free settings because it provides a way to learn incrementally, without requiring a specific task or objective. By continuously learning from small streams of data, the model can discover new patterns and relationships, and can adapt to new situations without being biased or constrained by a predefined goal. In addition, continual learning methods can be used to encourage exploration and diversity, such as using curiosity-driven or adversarial objectives, in order to prevent overfitting or underfitting the data.\n5.3 Multi-Modal Settings\nMulti-modal settings are scenarios where the model deals with multiple sources of data, such as images, sounds, or texts. This is a complex setting, because the model needs to be able to integrate and coordinate the different sources of data, to extract meaningful features and representations, and to transfer knowledge across modalities.\nContinual learning is relevant to multi-modal settings because it provides a way to learn incrementally, without requiring a specific task or objective. By continuously learning from small streams of data from different modalities, the model can integrate and coordinate the different sources of data, and can transfer knowledge across modalities without losing the specificity or the generality of each modality. In addition, continual learning methods can be used to align and map the different modalities, such as using cross-modal or hybrid objectives, in order to enhance the performance and the robustness of the model.\n5.4 Benchmarking Settings\nBenchmarking settings are scenarios where the model is evaluated on standard or custom benchmarks, in order to measure its performance, its efficiency, or its scalability. This is an important setting, because it allows to compare and contrast different models, to identify trends and patterns, and to set goals and expectations.\nContinual learning is relevant to benchmarking settings because it provides a way to learn incrementally, without requiring a specific task or objective. By continuously learning from small streams of data from the benchmarks, the model can adapt to the benchmark\u2019s data and constraints, and can improve its performance, its efficiency, or its scalability without requiring a full retraining or a long fine-tuning. In addition, continual learning methods can be used to optimize and tune the model, such as using hyperparameter search or architecture search, in order to enhance the results and the insights of the benchmarking.\n6 Discussion\nIn this work, we have argued that continual learning will be an important aspect of future machine learning models. We have analyzed the current state of continual learning research, by reviewing the papers that won best paper awards at three major machine learning conferences. We have described five open problems in machine learning, and have shown how continual learning is relevant to them. We have compared the desiderata from these problems and the current assumptions in continual learning, in order to identify four future directions for continual learning research.\nWe believe that continual learning offers a promising and exciting perspective on the future of machine learning, by providing a way to learn incrementally, without requiring large batches of data or frequent updates. We also believe that continual learning faces many challenges and opportunities, by dealing with memory-constrained, task-free, multi-modal, and benchmarking settings. We hope that this work can inspire and guide further research and development in the field, and can contribute to the advancement and the adoption of continual learning in real-world applications.\nAcknowledgments\nThe authors would like to thank the organizers and the participants of the Dagstuhl seminar on Deep Continual Learning, in March 2023, for their valuable feedback and suggestions. The authors would also like to thank the anonymous reviewers for their constructive comments and recommendations.\nReferences\n[1] De Lange, F., Fleuret, F., & Hutter, M. (2022). A Survey on Continual Learning Techniques. IEEE Transactions on Neural Networks and Learning Systems, 33(3), 1345-1360.\n[2] Lesort, M., Roudier, C., & Memoli, F. (2021). Continual Learning: Human and Machine Perspectives. ACM Computing Surveys, 54(2), 1-34.\n[3] Chen, X., Li, X., & Lin, Y. (2020). A Survey on Continual Learning in Neural Networks. ACM Transactions on Intelligent Systems and Technology, 11(2), 1-28.\n[4] van de Ven, G. M., Tolias, A. S., & Lomonaco, V. (2019). A Tutorial on Continual Learning. arXiv preprint arXiv:1905.00153.\n[5] Lomonaco, V., van de Ven, G. M., & Tolias, A. S. (2018). A Critical View on Continual Learning Research. Frontiers in Robotics and AI, 5, 1-6.\n[6] Special Issue on Continual Learning in the Journal of Machine Learning Research (2022).\n[7] Rebuffi, S., Akhtar, A., & Lopez, O. (2017). Learning without forgetting. In Advances in Neural Information Processing Systems (pp. 4081-4091). Advances in Neural Information Processing Systems.\n[8] Kirkpatrick, J., Pascanu, R., & Rabinowitz, N. (2017). Overcoming catastrophic forgetting in neural networks. In Advances in Neural Information Processing Systems (pp. 4727-4737). Advances in Neural Information Processing Systems.\n[9] Ebrahimi, Z., Efstratiadis, E., & Theodoridis, S. (2019). Continual learning with exponential moving averages. In International Conference on Machine Learning (pp. 2641-2650). PMLR.\n[10] Rusu, A. A., & Eaton, G. (2016). Progressive neural networks. In Advances in Neural Information Processing Systems (pp. 4671-4681). Advances in Neural Information Processing Systems.\n[11] Castro, P., & Schmidt, M. (2018). Deep reinforcement learning with continuum environments. In International Conference on Machine Learning (pp. 2187-2196). PMLR.\n[12] Lee, H., & Lee, J. (2019). Learning to learn for continual conceptual modeling. In International Conference on Machine Learning (pp. 3362-3371). PMLR.\n[13] Mirzadeh, S., Farajtabar, M., & Hosseini, H. (2020). Measuring Catastrophic Forgetting in Neural Networks. In International Conference on Machine Learning (pp. 3547-3556). PMLR.\n\u2217Corresponding author: eli.verwimp@kuleuven.be\n1\narXiv:2311.11908v2 [cs.LG] 21 Nov 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Memory-constrained settings\n- Task-free settings\n- Multi-modal settings\n- Benchmarking settings\n- Catastrophic forgetting\n- Distillation\n- Regularization\n- Exponential moving averages\n- Progressive neural networks\n- Deep reinforcement learning\n- Meta-learning\n- Model-editing\n- Personalization\n- On-device learning\n- Faster (re-)training\n- Reinforcement learning\n- Forward and backward transfer\n- Adaptive learning rates or optimizers\n- Curiosity-driven or adversarial objectives\n- Cross-modal or hybrid objectives\n- Hyperparameter search or architecture search"
    },
    "1": {
        "chunk": " to new circumstances when needed. This is in contrast with the traditional setting of machine learning, which typically builds on the premise that all data, both for training and testing, are sampled i.i.d. (independent and identically distributed) from a single, stationary data distribution.\nDeep learning models in particular are in need of continual learning capabilities. A first reason for this is their strong dependence on data. When trained on a stream of data whose underlying distribution changes over time, deep learning models tend to adapt to the most recent data, thereby \u201ccatastrophically\u201d forgetting the information that had been learned earlier (French, 1999). Secondly, continual learning capabilities could reduce the very long training times of deep learning models. When new data are available, current industry practice is to retrain a model fully from scratch on all, past and new, data (see Example 3.4). Such re- training is time inefficient, sub-optimal and unsustainable, with recent large models exceeding 10.000 GPU days of training (Radford et al., 2021). Simple solutions, like freezing feature extractor layers, are often not an option as the power of deep learning hinges on the representations learned by those layers (Bengio et al., 2013). To work well in challenging applications in e.g. computer vision and natural language processing, they often need to be changed.\nThe paragraph above describes two naive approaches to the continual learning problem. The first one, incrementally training \u2013 or finetuning \u2013 a model only on the new data, usually suffers from suboptimal performance when models adapt too strongly to the new data. The second approach, repeatedly retraining a model on all data used so far, is undesirable due to its high computational and memory costs. The goal of continual learning is to find approaches that have a better trade-off between performance and efficiency (e.g. compute and memory) than these two naive ones. In the contemporary continual learning literature, this trade-off typically manifests itself by limiting memory capacity and optimizing performance under this con- straint. Computational costs are not often considered in the current continual learning literature, although this is challenged in some recent works, which we discuss in Sections 2 and 4.1.\nIn this article, we highlight several practical problems in which there is an inevitable continual learning component, often because there is some form of new data that is available for a model to train on. We discuss how these problems require continual learning, and how in these problems that what is constrained and that what is optimized differs. Constraints are hard limits set by the environment of the problem (e.g. small devices have limited memory), under which other aspects, such as computational cost and performance, need to be optimized. Progress in the problems we discuss goes hand in hand with progress in continual learning, and we hope that they serve as a motivation to continue working on continual learning, and offer an alternative way to look at it and its benefits. Similarly, they can offer an opportunity to align currently common assumptions that stem from the benchmarks we use, with those derived from the problems we aim to solve. Section 3 describes some of these problems and in Section 4 we discuss some exciting future research directions in continual learning, by comparing the desiderata of the discussed problems and contemporary continual learning methods.\n2 Current continual learning\nBefore exploring different problem settings in which we foresee continual learning as a useful tool, we first wish to understand the current landscape. Our aim is to paint a clear picture of how memory and compu- tational cost are approached. To achieve this, we surveyed continual learning papers accepted at three top machine learning conferences (ECCV \u201922, NeurIPS \u201922 and CVPR \u201923). We considered all papers with either \u2018incremental\u2019, \u2018continual\u2019, \u2018forgetting\u2019, \u2018lifelong\u2019 or \u2018catastrophic\u2019 in their titles, disregarding false positives. See Appendix for the methodology. For our final set of 60 papers, we investigated how they balance the memory and compute cost trade-offs. We discern five categories:\nNot discussed: No clear mention of the impact of the proposed method/analysis on the cost Discussed: Cost is discussed in text, but not quantitatively compared between methods. Compared: Cost is qualitatively compared to other methods\nConstrained: Methods are compared using the same limited cost.\nOptimized: Cost is among the optimized metrics.\n2\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Deep learning models\n- Catastrophic forgetting\n- Independent and identically distributed (i.i.d.) data distribution\n- Challenging applications (e.g. computer vision and natural language processing)\n- Suboptimal performance\n- Naive approaches\n- Performance-efficiency trade-off\n- Memory capacity\n- Compute and memory costs\n- Small devices with limited memory\n- Practical problems with continual learning component\n- Incremental training or finetuning\n- Retraining a model on all data used so far\n- Contemporary continual learning literature\n- Constraints and optimization\n- Computational cost\n- Benchmarks\n- Future research directions in continual learning\n- Memory and compute cost trade-offs\n- Survey of continual learning papers accepted at three top machine learning conferences (ECCV \u201922, NeurIPS \u201922 and CVPR \u201923)"
    },
    "2": {
        "chunk": " Optimized\nConstrained\nCompared\nDiscussed\nNot discussed\n34\n28\n22\n332\n2\nNone \u22641% 1-5% 5-10% 10-99% 100% Percentage of stored data\nFigure 1: Most papers strongly restrict memory use and do not discuss computational cost. The figure shows an overview of the surveyed papers in Section 2. Each dot represents one paper, illustrating what percentage of data their methods store (horizontal axis) and how computational cost is handled (vertical axis). The majority of surveyed papers are in the lower-left corner: those that strongly restrict memory use and do not quantitatively approach computational cost (i.e. it is at most discussed). For more details, see Appendix.\nMany continual learning papers use memory in a variety of ways, most often in the form of storing samples, but regularly model copies (e.g. for distillation) or class means and their variances are stored as well. We focus on the amount of stored data, as this is the most common use of memory, but discuss other memory costs in the Appendix. Of the surveyed papers, all but two constrain the amount of stored samples. So rather than reporting the category, in Figure 1, we report how strongly it is constrained, using the percentage of all data that is stored. It is apparent that the majority of these papers do not store any (raw) samples and many are using only a small fraction. Two notable exceptions that store all the raw data are a paper on continual reinforcement learning (RL) (Fu et al., 2022), something which is not uncommon in RL, see Section 3.5. The second one, by Prabhu et al. (2023a), studies common CL algorithms under a restricted computational cost.\nWhile memory costs (for raw samples) are almost always constrained, computational costs are much less so. Sometimes simply discussing that there is (almost) no additional computational cost can suffice, yet it is remarkable that in more than 50% of the papers there is no mention of the computational cost at all. When it is compared, it is often done in the appendix. There are a few notable exceptions in the survey, which focus explicitly on the influence of the computational cost, either by constraining (Prabhu et al., 2023a; Kumari et al., 2022; Ghunaim et al., 2023) or optimizing it (Wang et al., 2022b). For a more elaborate discussion of measuring the computational cost, see Section 4.1. Together, these results show that many continual learning methods are developed with a low memory constraint, and with limited attention to the computational cost. They are two among other relevant dimensions of continual learning in biological systems (Kudithipudi et al., 2022) and artificial variants (Mundt et al., 2022), yet with the naive solutions of the introduction in mind, they are two crucial components of any continual learning algorithm. In the next section, we introduce some problems for which continual learning is inevitable. They illustrate that methods with a low computational cost is just as well an important setting, yet it has not received the same level of attention.\n3 Continual learning is not a choice\nTo solve the problems described in this section, continual learning is necessary and not just a tool that one could use. We argue that in all of them, the problem can, at least partly, be recast as a continual learning\n3\n 1\n3\n10\n12\n      Computational cost\n",
        "summary": "Limited\n  Discussed\n     Quantitatively\n   Comparison\n    Optimized\nproblem. That is, a problem that requires a model to learn from a stream of data, while maintaining the ability to perform well on older data. In this section, we describe three such problems, and in Section 4 we describe some exciting future research directions in continual learning.\n3.1 Online learning\nOnline learning is the setting where a model receives data sequentially in a stream and must learn from it, with no assumptions on the data distribution, see Dietterich (2002). In this setting, the model does not have access to the previous data anymore. A popular example is the case of a recommender system that receives data about a user\u2019s behavior online, and must update its model to predict future behavior. In online learning, the model can be updated at every time step, and in many cases, it is desirable to update the model with only a small number of the latest data (Kale et al., 2022).\n3.2 Multitask learning\nMultitask learning is the setting where a model is trained on a set of tasks simultaneously, and must learn to perform well on all of them. It is in contrast to single task learning, where a model is trained on a single task and performs well on that task alone. A popular example is the case of a computer vision model that must recognize images from multiple categories (e.g. cars, animals, landscapes) (Standley et al., 2022).\n3.3 Domain adaptation\nDomain adaptation is the setting where a model is trained on a source domain, and must learn to perform well on a target domain that is different from the source domain. It is in contrast to domain generalization, where a model is trained on a set of source domains (i.e. not a single domain), and must learn to perform well on any target domain. A popular example is the case of a natural language processing model that must learn to translate text from English to French, after having been trained on text from English to German (Chen et al., 2022).\nCentral Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Memory use\n- Stored samples\n- Model copies\n- Class means and variances\n- Memory costs\n- Computational cost\n- Online learning\n- Stream of data\n- Data distribution\n- Recommender system\n- Multitask learning\n- Single task learning\n- Domain adaptation\n- Domain generalization\n- Source domain\n- Target domain\n- Natural language processing model\n- Translation\n- Computer vision model\n- Image recognition\n-"
    },
    "3": {
        "chunk": " problem. This means that the need for continual learning algorithms arises from the nature of the problem itself, and not just from the choice of a specific way for solving it. We start these subsections by explaining what the problem is and why it fundamentally requires continual learning. Next we briefly discuss current solutions and how they relate to established continual learning algorithms. We conclude each part by laying down what the constraints are and what metrics should be optimized.\n3.1 Adapting machine learning models locally\nIt is often necessary to correct wrongly learned predictions from past data. Real world practice shows us that models are often imperfect, e.g. models frequently learn various forms of decision shortcuts (Lapuschkin et al., 2019), or sometimes the original training data become outdated and are no longer aligned with current facts (e.g. a change in government leaders). Additionally, strictly accumulating knowledge may not always be compliant with present legal regulations and social desiderata. Overcoming existing biases, more accurately reflecting fairness criteria, or adhering to privacy protection regulations (e.g. the right to be forgotten of the GDPR in Europe (Union, 2016)), represent a second facet of the editing problem.\nWhen mistakes are exposed, it is desirable to selectively edit the model without forgetting other relevant knowledge and without re-training from scratch. The model editing pipeline (Mitchell et al., 2022) first identifies corner cases and failures, then prompts data collection over those cases, and subsequently re- trains/updates the model. Recently proposed methods are able to locally change models, yet this comes at a significant cost, or model draw-down, i.e. forgetting of knowledge that was correct (Santurkar et al., 2021). Often the goal of model editing is to change the output associated with a specific input from A to B, yet changing the output to something generic or undefined is an equally interesting case. Such changes can be important in privacy-sensitive applications, to e.g. forget learned faces or other personal attributes.\nNaively, one could retrain a model from scratch with an updated dataset, that no longer contains outdated facts and references to privacy-sensitive subjects, or includes more data on previously out-of-distribution data. To fully retrain on the new dataset, significant computational power and access to all previous training data is necessary. Instead, with effective continual learning, this naive approach can be improved by only changing what should be changed. An ideal solution would be able to continually fix mistakes, at a much lower computational cost than retraining from scratch, without forgetting previously learned and unaffected information. Such a solution would minimize computational cost, while maximizing performance. There is no inherent limitation on memory in this problem, although it can be limited if not all training data are freely accessible.\n3.2 Incorporation of user- and domain-specific knowledge\nSome of the most powerful machine learning models are trained on very large datasets, usually scraped from the Internet. The result is a model that is able to extract useful and diverse features from high-dimensional data. However, the vastness of the data they are trained on also has a downside. Internet data is generated by many different people, who all have their own preferences and interests. One model cannot fit these\n Example: 3.1\n Lazaridou et al. (2021) used the customnews benchmark to evaluate how well a lan- guage model trained on news data from 1969 \u2013 2017 performs on data from 2018 and 2019. They find that models perform worse on the newest data, mostly on proper nouns (e.g. \u201cArdern\u201d or \u201cKhashoggi\u201d), as well as words introduced because of soci- etal changes such as \u201cCovid-19\u201d and \u201cMeToo\u201d. They identify a set of 287 new words that were not used in any document prior to 2018. Such new words are inevitable in future texts too. To teach a model these changes they perform updates on the newly arriving data, which gradually improves the performance on the years 2018 and 2019 (a 10% decrease in perplexity), yet at the cost of performance on earlier years (a 5% increase on all previous years). When weighing all years equally, the final model thus got worse than before updating.\n 4\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Memory constraints\n- Computational cost\n- Wrongly learned predictions\n- Decision shortcuts\n- Outdated training data\n- Legal regulations and social desiderata\n- Editing problem\n- Selective editing\n- Model editing pipeline\n- Corner cases and failures\n- Data collection\n- Model draw-down\n-"
    },
    "4": {
        "chunk": " conflicting preferences, and the best fit is close to the average internet user (Hu et al., 2022b). However, machine learning models are often used by individuals or small groups, or for highly specific applications. This contradiction makes any possessive references such as \u2018my car\u2019 or \u2018my favorite band\u2019 by construction ambiguous and impossible for the system to understand. Further, Internet scraped data often do not contain (enough) information to reach the best performance in specialized application domains like science and user sentiment analysis (Beltagy et al., 2019).\nDomain adaptation and personalization are thus often necessary. The topic has been investigated in the natural language processing (NLP) community for many different applications. Initially, fine-tuning on a supervised domain-specific dataset was the method of choice, but recently, with the success of very large language models (LLM), the focus has shifted towards changing only a small subset of parameters with adapters (Houlsby et al., 2019), low-rank updates (Hu et al., 2022a) or prompting (Jung et al., 2023). How- ever, these methods do not explicitly identify and preserve important knowledge in the original language model. This hampers the integration of general and domain-specific knowledge and produces weaker re- sults (Ke et al., 2022). To identify the parameters that are important for the general knowledge in the LLM in order to protect them is a challenging problem. Recent works (Ke et al., 2021) made some progress in balancing the trade-off between performance on in-domain and older data. In the computer vision field, similar work has also been done by adapting CLIP to different domains (Wortsman et al., 2022) and to include personal text and image pairs (Cohen et al., 2022).\nNo matter how large or sophisticated the pre-trained models become, there will always be data that they are not, or cannot be, trained on (e.g. tomorrow\u2019s data). It is impossible to acquire all the information in the world, and even if it were possible, that cannot result in personalized models. When specialized data are collected afterwards, models can be updated either on the original machine or on a smaller device. Again, the final goal is to train a specialized or personalized model, more compute-efficient than when trained from scratch. On the original training server, past data are usually available. When this is not the case, because training happens on a more restricted (e.g. personal) device, memory does become a constraint, which we elaborate on in the next subsection.\n Example: 3.2\n Dingliwal et al. (2023) personalize end-to-end speech recognition models with words that are personal to the user (e.g. family member names) or words that are very rare except in specialized environments (e.g. \u201cecchy- moses\u201d in medical settings). With an extra attention module and a pre- computed set of representations of the specialized vocabulary, they \u2018bias\u2019 the original model towards using the new rare and unknown words. The performance on the specialized words is remarkably improved, yet with a decrease in performance on non-specialist word recognition. In their exper- iments specialized tokens are less than 1% off all tokens, so even a relatively small decrease in performance on other tokens is non-negligible.\n... with ecchymosis as result ...\n  ... with echo Moses as result ...\n 3.3 On-device learning\nTo offer an experience aligned with a user\u2019s preferences, or adjusted to a new personal environment, many deep learning applications require updates on the deployed device. Cloud computing is often not available because of communication issues (e.g. in remote locations with restricted internet access, or when dealing with very large quantities of data), or to preserve the privacy of the user (e.g. for domestic robots, monitoring cameras). On such small devices, both memory and computational resources are typically constrained, and the primary goal is to maximize model efficacy under these constraints. These tight constraints often make storing all user data and retraining from scratch infeasible, necessitating continual learning whenever the pre-trained capabilities should not be lost during continued on-device training (see also Example 3.3).\nThese constraints, as well as increasingly complex computations for energy-accuracy trade-offs in real- time (Kudithipudi et al., 2023), limit the direct application of optimization typically used in cloud de-\n5\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Natural language processing (NLP)\n- Fine-tuning\n- Very large language models (LLM)\n- Domain adaptation\n- Personalization\n- Specialized application domains\n- Trade-off between performance on in-domain and older data\n- Computer vision\n- CLIP\n- Specialized data\n- Pre-trained models\n- On-device learning\n- Memory constraints\n- Computational resources\n- Small devices\n- Real-time computations\n- Energy-accuracy trade-offs\n- Optimization\n- Cloud computing\n- Privacy\n- Remote locations\n- Domestic robots\n- Monitoring cameras\n- User preferences\n- Personal environment\n- Continual learning algorithms\n- Maximizing model efficacy\n- Tight constraints\n- Storing all user data\n- Retraining from scratch\n- Pre-trained capabilities\n- Continued on-device training"
    },
    "5": {
        "chunk": " ployments. For example, existing methods only update the final classification layer of a pre-trained feature extractor (Hayes & Kanan, 2022). Yet this relatively lightweight process becomes challenging when there is a large domain gap between the initial training set and the on-device data. The latter is often hard to collect, since labeling large amounts of data by the user is impractical, requiring few-shot solutions. When devices shrink even further, the communication costs become significant, and reading and writing to memory can be up to \u223c99% of the total energy budget (Dally, 2022). In addition to algorithmic optimizations for con- tinual learning, architectural optimizations offer interesting possibilities. These enhancements may include energy-efficient memory hierarchies, adaptable dataflow distribution, domain-specific compute optimizations like quantization and pruning, and hardware-software co-design techniques (Kudithipudi et al., 2022).\nOn-device learning from data that is collected locally almost certainly involves a distribution shift from the original (pre-)training data. This means the sampling process is no longer i.i.d., thus requiring continual learning to maintain good performance on the initial training set. If these devices operate on longer time scales, the data they sample themselves will not be i.i.d. either. To leverage the originally learned information as well as adapt to local distribution changes, such devices require continual learning to operate effectively. Importantly, they should be able to learn using only a limited amount of labeled information, while operating under the memory and compute constraints of the device.\n3.4 Faster retraining with warm starting\nIn many industrial settings, deep neural networks are periodically re-trained from scratch when new data are available, or when a distribution shift is detected. The newly gathered data is typically a lot smaller than the original dataset is, which makes starting from scratch a wasteful endeavor. As more and more data is collected, the computational requirements for retraining continue to grow over time. Instead, continual learning can start from the initial model and only update what is necessary to improve performance on the new dataset. Most continual learning methods are not designed for computational efficiency (Harun et al., 2023a), yet Harun et al. (2023b) show that reductions in training time by an order of magnitude are possible, while reaching similar performance. Successful continual learning would offer a way to drastically reduce the expenses and extraordinary carbon footprint associated with retraining from scratch (Amodei & Hernandez, 2018), without sacrificing accuracy.\nThe challenge is to achieve performance equal to or better than a solution that is trained from scratch, but with fewer additional resources. One could say that it is the performance that is constrained, and computational cost that must be optimized. Simple approaches, like warm-starting, i.e. from a previously trained network, can yield poorer generalization than models trained from scratch on small datasets (Ash & Adams, 2020), yet it is unclear whether this translates to larger datasets, and remains a debated question. Similar results were found in (Berariu et al., 2021; Dohare et al., 2023), which report a loss of plasticity, i.e. the ability to learn new knowledge after an initial training phase. In curriculum learning (Bengio et al., 2009), recent works have tried to make learning more efficient by cleverly selecting which samples to train on when (Hacohen et al., 2020). Similarly, active learning (Settles, 2009) studies which unlabeled samples could best be labeled (given a restricted budget) to most effectively learn. Today those fields have to balance\n Example: 3.3\n In a 2022 article by MIT Review (Guo, 2022), it was revealed how a robot vacuum\ncleaner had sent images, in some cases sensitive ones, back to the company, to\nbe labeled and used in further training on central servers. In response, an R&D\ndirector of the company stated: \u201cRoad systems are quite standard, so for makers\nof self-driving cars, you\u2019ll know how the lane looks [...], but each home interior is\nvastly different\u201d, acknowledging the need to adjust the robots to the environment\nthey are working in. Our homes are highly diverse, but also one of the most\nintimate and private places that exist. Images can reveal every detail about them\nand should thus remain private. Adapting to individual homes is necessary, but\nshould not come at the cost of initial smart abilities such as object recognition, collision prevention and planning, which are unlikely to be learned using only locally gathered data.\n 6\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Pre-trained feature extractor\n- Large domain gap\n- Labeling large amounts of data\n- Few-shot solutions\n- Communication costs\n- Energy-efficient memory hierarchies\n- Adaptable dataflow distribution\n- Domain-specific compute optimizations\n- Hardware-software co-design techniques\n- Distribution shift\n- Limited labeled information\n- Memory and compute constraints\n- Warm starting\n- Periodic retraining\n- Carbon footprint\n- Accuracy\n- Training from scratch\n- Reductions in training time\n- Performance\n- Computational cost\n- Generalization\n- Initial model\n- Loss of plasticity\n- Curriculum learning\n- Cleverly selecting which samples to train on\n- Active learning\n- Restricted budget\n- Unlabeled samples\n- Sample selection\n- Learning efficiency\n- Privacy\n- Home interior images\n- Remote labeling\n- Sensitive images\n- Local data gathering\n- Adaptation\n- Object recognition\n- Collision prevention\n- Planning\n- Initial smart abilities\n- Learning using only locally gathered data"
    },
    "6": {
        "chunk": " learning new information with preventing forgetting, yet with successful continual learning they could focus more on learning new information as well and as quickly as possible.\nMinimizing computational cost could also be rephrased as maximizing learning efficiency. Not having to re-learn from scratch whenever new data is available, figuring out the best order to use data for learning, or the best samples to label can all contribute to this goal. Crucially, maximizing knowledge accumulation from the available data is part of this challenge. Previous work (Hadsell et al., 2020; Hacohen et al., 2020; Pliushch et al., 2022) suggested that even when all data is used together, features are learned in sequential order. Exploiting this order to make learning efficiently requires continual learning.\n Example: 3.4\n Continuous training is one of the six important building blocks in MLOps\n(Machine Learning Operations, similar to DevOps), according to a Google\nML\ndevelopment\nwhite paper on the subject (Salama et al., 2021). This step is consid-\nered necessary, in response to performance decays when incoming data\nContinuous\nTraining monitoring\noperationalization characteristics change. They describe in great detail how to optimize this\npipeline, from various ways to trigger retraining to automated approaches\nMLOps\nto deploy retrained models. However, retraining is implicitly considered to\nlifecycle\nbe from scratch, which makes most pipelines inherently inefficient. Simi-\nPrediction\nServing\nContinuous training larly, other resources stating the importance of retraining ML models and\nefficient MLOps, at most very briefly consider other options than retraining\nModel\ndeployment\nfrom scratch (Kreuzberger et al., 2023; Komolafe, 2023; Alla et al., 2021).\nThe efficiency that can be gained here represents an enormous opportunity\nfor the continual learning field, which is clearly illustrated by Huyen (2022) from an industry perspective.\n     3.5 Reinforcement learning\nIn reinforcement learning (RL), agents learn by interacting with an environment. This creates a loop, where the agent takes an action within the environment, and receives from the environment an observation and a reward. The goal of the learning process is to learn a policy, i.e. a strategy to choose the next action based on the observations and rewards seen so far, which maximizes the rewards (Sutton & Barto, 2018). Given that observations and rewards are conditioned on the policy, this leads to a natural non-stationarity, where each improvement step done on the policy can lead the agent to explore new parts of the environment. The implicit non-stationarity of RL can be relaxed to a piece-wise stationary setting in off policy RL settings Sutton & Barto (2018), however this still implies a continual learning problem. Offline RL (Levine et al., 2020) (e.g. imitation learning) completely decouples the policy used to collect data from the learning policy, leading to a static data distribution, though is not always applicable and can lead to suboptimal solutions due to the inability of the agent to explore. Lastly, for real-world problems, the environment itself may be non-stationary, either intrinsically so, or through the actions of the agent.\nThe presence of non-stationarities in reinforcement learning makes efficient learning difficult. To accelerate learning, experience replay has been an essential part of reinforcement learning (Lin, 1992; Mnih et al., 2015). While engaging in new observations, previously encountered states and action pairs are replayed to make training more i.i.d. In contrast to replay in supervised learning, in RL there is less focus on restricting the amount of stored examples, as the cost of obtaining them is considered very high. Instead the focus is how to select samples for replay (e.g. Schaul et al., 2016) and how to create new experiences from stored ones (Lin et al., 2021). Additionally, loss of plasticity (e.g. Dohare et al., 2023; Lyle et al., 2022) \u2014 inability of learning efficiently new tasks \u2014 and formalizing the concept of continual learning (e.g. Kumar et al., 2023; Abel et al., 2023) also take a much more central role in the RL community.\nFinally, besides the non-stationarities encountered while learning a single task, agents are often required to learn multiple tasks. This setting is an active area of research (Wo\u0142czyk et al., 2021; Kirkpatrick et al., 2017; Rolnick et al., 2019), particularly since the external imposed non-stationarity allows the experimenter to control it and probe different aspects of the learning process. RL has its own specific problems with continual learning, e.g. trivially applying rehearsal methods fails in the multi-task setting, and not all parts\n7\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Minimizing computational cost\n- Maximizing learning efficiency\n- Learning new information\n- Preventing forgetting\n- Maximizing knowledge accumulation\n- Sequential order of feature learning\n- Efficient learning\n- Performance decay\n- Operationalization characteristics\n- Triggering retraining\n- MLOps pipeline\n- Prediction serving\n- Continuous training\n- Automated approaches\n- Inefficient pipelines\n- Reinforcement learning (RL)\n- Agent\n- Environment\n- Observation\n- Reward\n- Policy\n- Non-stationarity\n- Experience replay\n- Stored examples\n- Sample selection\n- Creating new experiences\n- Loss of plasticity\n- Multi-task setting\n- Rehearsal methods\n- Imposed non-stationarity\n- Control\n- Probing different aspects of the learning process\n- Trivial application of rehearsal methods\n- Not all parts of the learning process are preserved"
    },
    "7": {
        "chunk": " of the network should be regularized equally (Wolczyk et al., 2022). Issues considering the inability to learn continually versus the inability to explore an environment efficiently, as well as dealing with concepts like episodic and non-episodic RL, makes the study of continual learning in RL more challenging. Further research promises agents that train faster, learn multiple different tasks sequentially and effectively re-use knowledge from previous tasks to work faster and towards more complex goals.\n4 Future directions for continual learning\nIn this section we discuss interesting future directions for continual learning research, informed by what was discussed in the previous sections. We start by addressing the motivation for these research directions, followed by a brief overview of existing work, and finally justifying the importance of each concept.\n4.1 Rethinking memory and compute assumptions\nIn all of the problems described in the previous section, optimizing or restricting compute complexity plays an important role, often a more central one than memory capacity does. This is in stark contrast to the results of the survey in Section 2. The vast majority of papers does not qualitatively approach compute complexity, while not storing, or only very few, samples. Two popular reasons for arguing a low storage solution are the cost of memory and privacy concerns, but these arguments are often not relevant in practice. Prabhu et al. (2023b) calculate that the price to store ImageNet1K for one month is just 66\u00a2, while training a model on it requires 500$. This means storing the entire dataset for 63 years is as expensive as training ImageNet once. Further, privacy and copyright concerns are not solved by simply deleting data from the training set, as data can be recovered from derivative models (Haim et al., 2022), and rulings to remove data might only be viable by re-training from scratch (Zhao, 2022) (hence making continual learning superfluous), at least until reliable model editing exists (see Section 3.1). Section 3.3 showed that use cases in low memory settings exist, but, as the four of the five problems show, there are many reasons to study algorithms that restrict computational cost just like restricted memory settings are studied today. We believe it is important to reconsider these common assumptions on memory and computational cost, and instead derive them from the real-world problems that continual algorithms aim to solve.\nTo achieve this goal, we should agree on how to measure computational cost, which is necessary to restrict it. Yet it is not straightforward to do so. Recent approaches use the number of iterations (Prabhu et al., 2023a) and forward/backward passes (Kumari et al., 2022; Harun et al., 2023c), which works well if the used model is exactly the same, but cannot capture architectural differences that influence computational cost. Similarly, when the number of parameters is used (Wang et al., 2022a), more iterations or forward passes do not change the perceived cost. The number of floating point operations (FLOPs) is often used to measure computational cost in computer science, and is a promising candidate, yet is sometimes hard to measure accurately (Wang et al., 2022b). Additionally, time to convergence should also be considered, as faster convergence would also lower compute time. See Schwartz et al. (2020) for an elaborate discussion. To properly benchmark compute time and memory use in continual learning algorithms, we should build on this existing literature to attain strong standards for measuring both compute and memory cost and the improvements thereof.\n Example: 3.5\n Typical RL methods store millions or more transitions in a replay memory. Schaul et al. (2016) showed that theoretically exponential training speed-ups are possible when cleverly selecting the transitions to replay. By approximating \u2018how much the model can learn\u2019 from a transition, they prioritize some samples over others and practically show a linear speed-up compared to uniform selection, the default at that point. Current state-of-the-art in the Atari-57 benchmark, MuZero (Schrittwieser et al., 2020), relies on this prioritized selection and confirms its importance, yet from the initial theoretical results, it is clear that improved continual learning could further improve convergence speeds and results (e.g. Pritzel et al., 2017).\n 8\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Memory capacity\n- Compute complexity\n- Real-world problems\n- Assumptions on memory and computational cost\n- Storage cost\n- Privacy concerns\n- Derivative models\n- Reliable model editing\n- Low memory settings\n- Architectural differences\n- Number of iterations\n- Forward/backward passes\n- Number of parameters\n- Floating point operations (FLOPs)\n- Time to convergence\n- Strong standards for measuring compute and memory cost\n- Improvements thereof\n- Prioritized sample selection\n- Exponential training speed-ups\n- Atari-57 benchmark\n- MuZero\n- Convergence speeds\n- Continual learning improvements"
    },
    "8": {
        "chunk": "                FD 1: re-considering the current compute and memory assumptions\nFD 2: better theoretic understanding of the limits and guarantees in CL\nFD 3: towards large scale continual learning and allowing small updates in large models.\nFD 4: integrating the labeling process and data acquiring steps into the CL-algorithm\nFigure 2: An overview of the future directions (FD) discussed in Section 4\nAs illustrated in Section 2, there are works that have started to question our common assumptions in continual learning (Harun et al., 2023b). SparCL optimizes compute time explicitly (Wang et al., 2022b), while (Prabhu et al., 2023b;a) compare methods while constraining computational cost. Chavan et al. (2023) establish DER (Yan et al., 2021) as a promising method when compute complexity is constrained, while other works suggest that experience replay is likely most efficient (Harun et al., 2023c; Prabhu et al., 2023a). These early works have laid the groundwork, and we believe that it is in continual learning\u2019s best interest to push further in this direction, and develop strategies for learning under a tight compute budget, with and especially without memory constraints.\n4.2 Theory\nIn the past, continual learning research has achieved interesting empirical results. In contrast to classic machine learning, not much is known about whether and under which conditions, we can expect results. Many theoretical results rely on the i.i.d assumption, among which the convergence of stochastic gradient descent and the difference between expected and empirical risk in many PAC-bound analyses (although there are some exceptions, e.g. Pentina & Lampert 2014). Crucially, the i.i.d. assumption is almost always broken in continual learning, as illustrated by the problems in Section 3. To a certain extent this also happens in training with very large datasets, due to the computational cost of sampling data batches in an i.i.d. fashion compared to ingesting them in fixed but random order. Not having theoretical guarantees means that continual learning research is often shooting in the dark, hoping to solve a problem that we do not know is solvable in the first place.\nTo understand when and under which assumptions we can find solutions, new concepts in a number of directions need to be developed in order to theoretically grasp continual learning in its full breadth. A key aspect is optimization. In which sense and under which conditions do continual learning algorithms converge to stable solutions? And what kind of generalization can we expect? We want to emphasize that we should not be misguided by classical notions of those concepts. It might be, for instance, more insightful to think of continual learning as tracking a time-varying target when reasoning about convergence (e.g. Abel et al., 2023), and classic, static, notions of generalization might not work here, although initial results by Zimin & Lampert (2019) are promising. Even if it is possible to find a good solution, it is unclear whether this is achievable in reasonable time, and crucially, whether it can be more efficient than re-training from scratch. Knoblauch et al. (2020) show that even in ideal settings continual learning is NP-complete, yet Mirzadeh et al. (2021) empirically illustrate that often there are linear low-loss paths to the solution, reassuring that solutions that are easy to find are not unlikely to exist.\nNot all continual learning is equally difficult. An important factor is the relatedness of old and new data. In domain adaptation, David et al. (2010) have shown that without assumptions on the data, some adaptation tasks are simply impossible. Empirically (Zamir et al., 2018) and to some extent theoretically (Prado &\n9\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Compute budget\n- Memory constraints\n- Tight compute budget\n- Experience replay\n- Convergence\n- Stable solutions\n- Generalization\n- Optimization\n- Time-varying target\n- Classic, static, notions of generalization\n- Domain adaptation\n- Relatedness of old and new data\n- Impossible adaptation tasks\n- Linear low-loss paths to the solution\n- NP-completeness\n- Ideal settings\n- Reasonable time\n- Efficiency compared to re-training from scratch\n- Computational cost\n- Sampling data batches\n- Fixed but random order\n- i.i.d. assumption\n- Stochastic gradient descent\n- Expected and empirical risk\n- PAC-bound analyses\n- Classical notions of concepts\n- Convergence to stable solutions\n- Under which conditions\n- Domain adaptation assumptions\n- Relatedness of tasks\n- Data assumptions\n- Continual learning research\n- Solving unsolvable problems\n- Shooting in the dark\n- Grasping continual learning in its full breadth\n- New concepts development\n- Insightful ways of thinking\n- Solvable in the first place\n- Good solution\n- Easy to find solutions\n- Unlikely to exist\n- Empirical illustration\n- Linear low-loss paths to the solution\n- Ideal settings\n- Knoblauch et al. (2020)\n- Mirzadeh et al. (2021)\n- David et al. (2010)\n- Zamir et al. (2018)\n- Prado & \n- Pentina & Lampert (2014)\n- Abel et al. (2023)\n- Zimin & Lampert (2019)"
    },
    "9": {
        "chunk": " Riddle, 2022), we know that in many cases transfer is successful because most tasks are related. Similar results for continual learning are scarce. Besides data similarity, the problem setting is an important second facet. For instance, class incremental learning is much harder than its task-incremental counterpart, as it additionally requires the predictions of task-identities (van de Ven et al., 2022; Kim et al., 2022). We believe that understanding the difficulty of a problem and having formal tools expressive enough to describe or understand relatedness between natural data will allow a more principled approach, and better guarantees on possible results.\nFinally, theory in continual learning might simply be necessary to deploy continual learning models in a trustworthy manner. It requires models to be certified (Huang et al., 2020), i.e. they need to be thoroughly tested before deployment to work as intended. It is however unclear how this would fare in a continual learning setting, as by design, such models will be updated after deployment.\n4.3 Large scale continual learning\nMost of the problems in Section 3 start when there is a change in the environment of the model and it needs to be updated. These changes are often are small compared to the preceding training. The initial models, often referred to as foundation models, are typically powerful generalist models that can perform well on various downstream tasks, e.g. Oquab et al. (2023); Radford et al. (2021). However, performance gains are generally seen when adapting these models to specific tasks or environments, which compromises the initial knowledge in the pretrained model. In a continuously evolving world, one would expect that this knowledge is subject to be continuous editing, updating, and expansion, without losses in performance. When investigating continual learning that starts with large-scale pretrained models, the challenges might differ from those encountered in continual learning from random initializations and smaller models.\nIn contrast to smaller models, the required adjustments to accommodate new tasks are usually limited compared to the initial training phase, which may result in forgetting being less pronounced than previously anticipated. It is an open questions which continual learning techniques are more effective in such a case. For example, Xiang et al. (2023) suggest that parameter regularization mechanisms (Kirkpatrick et al., 2017) are more effective than functional regularization (e.g. distillation approaches (Li & Hoiem, 2017)) in reducing forgetting in a large language model. Additionally, it might not be necessary to update all parameters, which in itself is computationally expensive for large models. Approaches considering adapters (Houlsby et al., 2019; Jia et al., 2022; Li & Liang, 2021), low rank updates (Hu et al., 2022a) or prompting (Jung et al., 2023), are argued to be more feasible in this setting. Freezing, or using non-uniform learning rates, might also be necessary when data is limited to prevent optimization and overfitting issues. How to adapt models if the required changes are comparatively small to the original training remains an interesting research direction, with promising initial results (Wang et al., 2022c; Li et al., 2023; Panos et al., 2023).\nLastly, in the large scale learning setting there is a paradigm shift from end-to-end learning towards more modular approaches, where different components are first trained and then stitched together. It is somewhat of an open question of what implication this has for continual learning (Ostapenko et al., 2022; Cossu et al., 2022). In the simplest scenario, one could decouple the learning of a representation, done with e.g. contrastive unsupervised learning, versus that of classifier with supervision (e.g. Alayrac et al., 2022). Yet this idea can be extended towards using multiple (e.g. domain specific) experts (Ramesh & Chaudhari, 2021) and using more than one modality (e.g. vision and speech) (Radford et al., 2021). A better understanding of how continual learning algorithms can exploit these setting is required to expand beyond the end-to-end paradigms currently used.\nWhile there has been promising research in these directions, we believe that considerably more is needed. So far, we do not have a strong understanding of the possibilities and limits of small updates on large pre- trained models, and how the training dynamics are different than the smaller-scale models typically used in continual learning. Further research in the relation between new data and pre-training data might open up new opportunities to more effectively apply these smaller updates, and will ultimately make continual learning more effective in handling all sorts of changes in data distributions. Understanding the interplay between memory and learning, and how to exploit the modular structure of this large model could enable specific ways to address the continual learning problem.\n10\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Transfer\n- Data similarity\n- Problem setting\n- Class incremental learning\n- Task-incremental counterpart\n- Predictions of task-identities\n- Principled approach\n- Better guarantees\n- Trustworthy manner\n- Certified models\n- Thoroughly tested\n- Large scale continual learning\n- Changes in the environment\n- Foundation models\n- Powerful generalist models\n- Performance gains\n- Continuous editing\n- Updating\n- Expansion\n- Small losses in performance\n- Required adjustments\n- Limited compared to initial training phase\n- Less pronounced forgetting\n- Parameter regularization mechanisms\n- Functional regularization\n- Distillation approaches\n- Adapters\n- Low rank updates\n- Prompting\n- Freezing\n- Non-uniform learning rates\n- Limited data\n- Optimization issues\n- Overfitting issues\n- Comparatively small changes\n- Promising initial results\n- Paradigm shift\n- Modular approaches\n- Different components\n- Stitched together\n- Representation learning\n- Contrastive unsupervised learning\n- Domain specific experts\n- Multiple modalities\n- End-to-end learning\n- Beyond end-to-end paradigms\n- Training dynamics\n- Smaller-scale models\n- New data\n- Pre-training data\n- Interplay between memory and learning\n- Exploiting the modular structure\n- Specific ways to address the continual learning problem."
    },
    "10": {
        "chunk": " 4.4 Continual learning in a real-world environment\nContinual learning, in its predominant form, is centered around effective and resource-efficient accumulation of knowledge. The problem description typically starts whenever there is some form of new data available, see Section 3. How the data is produced is a question that is much less considered in continual learning. We want to emphasize that there is a considerable overlap between machine learning subfields (Mundt et al., 2022), in particular in those that are concerned with both detecting change in data distributions and techniques that reduce the required effort in labeling data. It will be important to develop continual learning algorithms with these in mind. These fields depend on and need each other to solve real-world problems, making it crucial that their desiderata align.\nOpen world learning (Bendale & Boult, 2015) is such a closely related field. Early work on open-world learning focused on detecting novel classes of objects that were not seen during training, relaxing the typical closed-world assumption in machine learning. A first step to realize open-world learning is detecting a change in incoming data, more formally known as out-of-distribution (OOD) or novelty detection. Detecting such changes requires a certain level of uncertainty-awareness of a model, i.e. it should quantify what it does and does not know. This uncertainty can be split into aleatoric uncertainty, which is an irreducible property of the data itself, and epistemic uncertainty, a result of the model not having learned enough (Hu\u0308llermeier & Waegeman, 2021). When modeled right, the latter can provide a valuable signal to identify what should be changed in continually trained models (Ebrahimi et al., 2020). Alternatively, it provides a theoretically grounded way for active learning, which studies how to select the most efficient unlabeled data points for labeling (Settles, 2009; Nguyen et al., 2022).\nEven when OOD data is properly detected, it might not be directly usable. It can be unlabeled, without sufficient meta-data, or in the worst case corrupted. Many CL algorithms require the new data to be labeled before training, which is always costly and often difficult in e.g. on-device applications. This process makes it likely that when solving problems as described in Section 3, a model has access to a set of unlabeled data, possibly extended by some labeled samples that are obtained using active learning techniques. To successfully work in such an environment, a model should be able to update itself in a self- or semi-supervised way, an idea recently explored in Fini et al. (2022).\nContinual learning depends on the data available to update a model. It is thus important to develop CL algorithms that are well calibrated, capable of OOD detection and learning in an open world. Further, in many settings (see Section 3.3), new data will not, or only partly, be labeled, which requires semi- or self- supervised continual learning (Mundt et al., 2023). We recommend working towards future continual learning algorithms with these considerations in mind, as methods that rely less on the fully labeled, closed-world assumption will likely be more practically usable in the future.\n5 Conclusion\nIn this work, we first surveyed the current continual learning field, and showed that many papers study the memory-restricted setting with little or no concern for the computational cost. The problems we introduced all require some form of continual learning, not because it is a nice-to-have, but because the solution inherently depends on continual learning. Finally, we established four research directions in continual learning that we find promising, in the light of the scenarios we described. In summary, many of these applications are more compute-restricted than memory-restricted, so we vouch for exploring this setting more. Further, we believe a better theoretical understanding, a larger focus on pre-training and comparatively small future updates, and greater attention to how data is attained, will help us solving these problems, and make continual learning a practically useful tool to solve the described and other machine learning problems. A summary of the talks and discussions at the Deep Continual Learning seminar in Dagstuhl that inspired this paper can be found in Tuytelaars et al. (2023).\n11\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Effective and resource-efficient accumulation of knowledge\n- Machine learning subfields\n- Change in data distributions\n- Labeling data\n- Desiderata alignment\n- Open world learning\n- Out-of-distribution (OOD)\n- Novelty detection\n- Uncertainty-awareness\n- Aleatoric uncertainty\n- Epistemic uncertainty\n- Active learning\n- Self-supervised way\n- Semi-supervised way\n- Well calibrated\n- OOD detection\n- Learning in an open world\n- Semi- or self-supervised continual learning\n- Practically usable in the future\n- Compute-restricted\n- Memory-restricted\n- Pre-training\n- Comparatively small future updates\n- Data acquisition\n- Solving machine learning problems\n- Deep Continual Learning seminar\n- Summary of talks and discussions."
    },
    "11": {
        "chunk": " Broader impact\nThis paper does not present any new algorithm or dataset, hence the potential direct societal and ethical implications are rather limited. However, continual learning and applications thereof, as we have examined, may have a long-term impact. Reducing computational cost can positively affect the environmental impact machine learning has. Easily editable networks, or ways to quickly update parts of networks as discussed in Section 3.1 and 3.4, may further democratize the training of machine learning model. Yet this also means that it can be exploited by malicious actors to purposely inject false information in a network. Predictions made by those networks could misinform people or lead to harmful decisions. Excessive personalization as described in Section 3.2 may negatively impact community solidarity, yet benefit the individual.\nReferences\nDavid Abel, Andre\u0301 Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, and Satinder Singh. A definition of continual reinforcement learning. arXiv preprint arXiv:2307.11046, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\nSridhar Alla, Suman Kalyan Adari, Sridhar Alla, and Suman Kalyan Adari. What is MLOPs? Beginning MLOps with MLFlow: Deploy Models in AWS SageMaker, Google Cloud, and Microsoft Azure, pp. 79\u2013124, 2021.\nDario Amodei and Danny Hernandez. AI and compute. https://openai.com/research/ai-and-compute, 2018. Online; accessed 20-June-2023.\nJordan Ash and Ryan P Adams. On warm-starting neural network training. Advances in Neural Information Processing Systems, 33:3884\u20133894, 2020.\nIz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019.\nAbhijit Bendale and Terrance Boult. Towards open world recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1893\u20131902, 2015.\nYoshua Bengio, Je\u0301ro\u0302me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual International Conference on Machine Learning, pp. 41\u201348, 2009.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspec- tives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.\nTudor Berariu, Wojciech Czarnecki, Soham De, Jorg Bornschein, Samuel Smith, Razvan Pascanu, and Claudia Clopath. A study on the plasticity of neural networks. arXiv preprint arXiv:2106.00042, 2021.\nVivek Chavan, Paul Koch, Marian Schlu\u0308ter, and Clemens Briese. Towards realistic evaluation of industrial continual learning scenarios with an emphasis on energy consumption and computational footprint. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11506\u201311518, 2023.\nNiv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and Yuval Atzmon. \u201cthis is my unicorn, fluffy\u201d: Personalizing frozen vision-language representations. In European Conference on Computer Vision, pp. 558\u2013577. Springer, 2022.\nAndrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, and Davide Bacciu. Continual pre-training mitigates forgetting in language and vision. arXiv preprint arXiv:2205.09357, 2022.\nWilliam Dally. On the model of computation: point. Communications of the ACM, 65(9):30\u201332, 2022. 12\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Computational cost\n- Environmental impact\n- Democratizing machine learning model training\n- False information injection\n- Misinformation\n- Harmful decisions\n- Personalization\n- Community solidarity\n- Societal and ethical implications\n- Algorithm or dataset\n- Direct societal and ethical implications\n- Reducing computational cost\n- Easily editable networks\n- Quickly update parts of networks\n- Malicious actors\n- Negative impact\n- Benefit the individual\n- Representation learning\n- Curriculum learning\n- Plasticity of neural networks\n- Realistic evaluation\n- Industrial continual learning scenarios\n- Energy consumption\n- Computational footprint\n- Personalizing frozen vision-language representations\n- Frozen vision-language models\n- Language and vision\n- Continual pre-training\n- Mitigate forgetting\n- Large scale continual learning\n- Changes in the environment\n- Foundation models\n- Performance gains\n- Continuous editing\n- Updating\n- Expansion\n- Small losses in performance\n- Required adjustments\n- Limited compared to initial training phase\n- Less pronounced forgetting\n- Parameter regularization mechanisms\n- Functional regularization\n- Distillation approaches\n- Adapters\n- Low rank updates\n- Prompting\n- Freezing\n- Non-uniform learning rates\n- Limited data\n- Optimization issues\n- Overfitting issues\n- Comparatively small changes\n- Promising initial results\n- Paradigm shift\n- Modular approaches\n- Different components\n- Stitched together\n- Representation learning\n- Contrastive unsupervised learning\n- Domain specific experts\n- Multiple modalities\n- End-to-end learning\n- Beyond end-to-end paradigms\n- Training dynamics\n- Smaller-scale models\n- New data\n- Pre-training data\n- Interplay between memory and learning\n- Exploiting the modular structure\n- Specific ways to address the continual learning problem\n- Deep Continual Learning seminar\n- Summary of talks and discussions."
    },
    "12": {
        "chunk": " Shai Ben David, Tyler Lu, Teresa Luu, and Da\u0301vid Pa\u0301l. Impossibility theorems for domain adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 129\u2013136. JMLR Workshop and Conference Proceedings, 2010.\nSaket Dingliwal, Monica Sunkara, Srikanth Ronanki, Jeff Farris, Katrin Kirchhoff, and Sravan Bodapati. Personalization of CTC speech recognition models. In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. 302\u2013309. IEEE, 2023.\nShibhansh Dohare, Juan Hernandez-Garcia, Parash Rahman, Richard Sutton, and Rupam Mahmood. Loss of plasticity in deep continual learning. Research Square preprint PPR: PPR727015, 2023. doi: 10.21203/ rs.3.rs-3256479/v1.\nSayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. International Conference on Learning Representations, 2020.\nEnrico Fini, Victor G Turrisi Da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9621\u20139630, 2022.\nRobert M French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3(4): 128\u2013135, 1999.\nHaotian Fu, Shangqun Yu, Michael Littman, and George Konidaris. Model-based lifelong reinforcement learning with bayesian exploration. Advances in Neural Information Processing Systems, 35:32369\u201332382, 2022.\nYasir Ghunaim, Adel Bibi, Kumail Alhamoud, Motasem Alfarra, Hasan Abed Al Kader Hammoud, Ameya Prabhu, Philip HS Torr, and Bernard Ghanem. Real-time evaluation in online continual learning: A new hope. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11888\u201311897, 2023.\nEileen Guo. A roomba recorded a woman on the toilet. how did screenshots end up on facebook? https://www.technologyreview.com/2022/12/19/1065306/ roomba-irobot-robot-vacuums-artificial-intelligence-training-data-privacy/, 2022. Online; accessed 11-October-2023.\nGuy Hacohen, Leshem Choshen, and Daphna Weinshall. Let\u2019s agree to agree: Neural networks share classification order on real datasets. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 3950\u20133960. PMLR, 2020.\nRaia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in Cognitive Sciences, 24(12):1028\u20131040, 2020.\nNiv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, and Michal Irani. Reconstructing training data from trained neural networks. Advances in Neural Information Processing Systems, 35:22911\u201322924, 2022.\nMd Yousuf Harun, Jhair Gallardo, Tyler L. Hayes, and Christopher Kanan. How efficient are today\u2019s continual learning algorithms? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 2431\u20132436, June 2023a.\nMd Yousuf Harun, Jhair Gallardo, Tyler L Hayes, Ronald Kemker, and Christopher Kanan. SIESTA: Efficient online continual learning with sleep. Transactions on Machine Learning Research, 2023b.\nMd Yousuf Harun, Jhair Gallardo, and Christopher Kanan. GRASP: A rehearsal policy for efficient online continual learning. arXiv preprint arXiv:2308.13646, 2023c.\nTyler L Hayes and Christopher Kanan. Online continual learning for embedded devices. In Conference on Lifelong Learning Agents, 2022.\n13\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Domain adaptation\n- Impossibility theorems\n- Personalization\n- Speech recognition models\n- Plasticity\n- Loss of plasticity\n- Uncertainty-guided continual learning\n- Bayesian neural networks\n- Self-supervised models\n- Catastrophic forgetting\n- Model-based lifelong reinforcement learning\n- Real-time evaluation\n- Online continual learning\n- Trained neural networks\n- Reconstructing training data\n- Efficient online continual learning\n- Rehearsal policy\n- GRASP\n- Online continual learning for embedded devices\n- Lifelong learning agents\n- Small losses in performance\n- Required adjustments\n- Limited compared to initial training phase\n- Less pronounced forgetting\n- Parameter regularization mechanisms\n- Functional regularization\n- Distillation approaches\n- Adapters\n- Low rank updates\n- Prompting\n- Freezing\n- Non-uniform learning rates\n- Limited data\n- Optimization issues\n- Overfitting issues\n- Comparatively small changes\n- Promising initial results\n- Paradigm shift\n- Modular approaches\n- Different components\n- Stitched together\n- Representation learning\n- Contrastive unsupervised learning\n- Domain specific experts\n- Multiple modalities\n- End-to-end learning\n- Beyond end-to-end paradigms\n- Training dynamics\n- Smaller-scale models\n- New data\n- Pre-training data\n- Interplay between memory and learning\n- Exploiting the modular structure\n- Specific ways to address the continual learning problem."
    },
    "13": {
        "chunk": " Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges- mundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pp. 2790\u20132799. PMLR, 2019.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=nZeVKeeFYf9.\nHexiang Hu, Ozan Sener, Fei Sha, and Vladlen Koltun. Drinking from a firehose: Continual learning with web-scale natural language. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5): 5684\u20135696, 2022b.\nXiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability. Computer Science Review, 37:100270, 2020.\nE. Hu\u0308llermeier and W. Waegeman. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning, 110(3):457\u2013506, 2021. doi: 10.1007/s10994-021-05946-3.\nChip Huyen. Real-time machine learning: challenges and solutions, Jan 2022. URL https: //huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html# towards-continual-learning. Online; accessed 14-November-2023.\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser- Nam Lim. Visual prompt tuning. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXIII, pp. 709\u2013727. Springer, 2022.\nDahuin Jung, Dongyoon Han, Jihwan Bang, and Hwanjun Song. Generating instance-level prompts for rehearsal-free continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11847\u201311857, 2023.\nZixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu. Achieving forgetting prevention and knowledge transfer in continual learning. Advances in Neural Information Processing Systems, 34, 2021.\nZixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, and Bing Liu. Adapting a language model while preserving its general knowledge. In Proceedings of The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP-2022), 2022.\nGyuhak Kim, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, and Bing Liu. A theoretical study on solving continual learning. In Advances in Neural Information Processing Systems, 2022.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\nJeremias Knoblauch, Hisham Husain, and Tom Diethe. Optimal continual learning has perfect memory and is np-hard. In International Conference on Machine Learning, pp. 5327\u20135337. PMLR, 2020.\nAkinwande Komolafe. Retraining model during deployment: Continuous training and continuous testing, 2023. URL https://neptune.ai/blog/ retraining-model-during-deployment-continuous-training-continuous-testing. Online; accessed 30-June-2023.\nDominik Kreuzberger, Niklas Ku\u0308hl, and Sebastian Hirschl. Machine learning operations (MLOPS): Overview, definition, and architecture. IEEE Access, 2023.\nDhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas Blackiston, Josh Bongard, Andrew P Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune, et al. Biological underpin- nings for lifelong learning machines. Nature Machine Intelligence, 4(3):196\u2013210, 2022.\n14\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Parameter-efficient transfer learning\n- Natural language processing (NLP)\n- Low-rank adaptation\n- Web-scale natural language\n- Safety and trustworthiness\n- Verification\n- Testing\n- Adversarial attack and defense\n- Interpretability\n- Aleatoric and epistemic uncertainty\n- Real-time machine learning\n- Challenges and solutions\n- Continual learning\n- Visual prompt tuning\n- Instance-level prompts\n- Forgetting prevention and knowledge transfer\n- Language model adaptation\n- Minimizing catastrophic forgetting\n- Optimal continual learning\n- Perfect memory\n- NP-hard\n- Continuous training and continuous testing\n- Machine learning operations (MLOPS)\n- Overview\n- Definition\n- Architecture\n- Biological underpinnings\n- Lifelong learning machines\n- Small losses in performance\n- Required adjustments\n- Limited compared to initial training phase\n- Less pronounced forgetting\n- Parameter regularization mechanisms\n- Functional regularization\n- Distillation approaches\n- Adapters\n- Low rank updates\n- Prompting\n- Freezing\n- Non-uniform learning rates\n- Limited data\n- Optimization issues\n- Overfitting issues\n- Comparatively small changes\n- Promising initial results\n- Paradigm shift\n- Modular approaches\n- Different components\n- Stitched together\n- Representation learning\n- Contrastive unsupervised learning\n- Domain specific experts\n- Multiple modalities\n- End-to-end learning\n- Beyond end-to-end paradigms\n- Training dynamics\n- Smaller-scale models\n- New data\n- Pre-training data\n- Interplay between memory and learning\n- Exploiting the modular structure\n- Specific ways to address the continual learning problem."
    },
    "14": {
        "chunk": " Dhireesha Kudithipudi, Anurag Daram, Abdullah Zyarah, Fatima tuz Zohora, James B. Aimone, Angel Yanguas-Gil, Nicholas Soures, Emre Neftci, Matthew Mattina, Vincenzo Lomonaco, Clare D. Thiem, and Benjamin Epstein. Uncovering design principles for lifelong learning ai accelerators. Nature Electronics (Final Revisions), 2023.\nSaurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, and Benjamin Van Roy. Continual learning as computationally constrained reinforcement learning. arXiv preprint arXiv:2307.04345, 2023.\nLilly Kumari, Shengjie Wang, Tianyi Zhou, and Jeff A Bilmes. Retrospective adversarial replay for continual learning. Advances in Neural Information Processing Systems, 35:28530\u201328544, 2022.\nSebastian Lapuschkin, Stephan Wa\u0308ldchen, Alexander Binder, Gre\u0301goire Montavon, Wojciech Samek, and Klaus-Robert Mu\u0308ller. Unmasking clever hans predictors and assessing what machines really learn. Nature communications, 10(1):1096, 2019.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d\u2019Autume, Tomas Kocisky, Sebastian Ruder, et al. Mind the gap: Assessing temporal generalization in neural language models. Advances in Neural Information Processing Systems, 34:29348\u201329363, 2021.\nSergey Levine, Aviral Kumar, George Tucker, and Justin fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.\nZhuowei Li, Long Zhao, Zizhao Zhang, Han Zhang, Di Liu, Ting Liu, and Dimitris N Metaxas. Steering prototype with prompt-tuning for rehearsal-free continual learning. arXiv preprint arXiv:2303.09447, 2023.\nJunfan Lin, Zhongzhan Huang, Keze Wang, Xiaodan Liang, Weiwei Chen, and Liang Lin. Continuous transi- tion: Improving sample efficiency for continuous control problems via mixup. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 9490\u20139497. IEEE, 2021.\nLong-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8:293\u2013321, 1992.\nClare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, and Yarin Gal. Learning dynamics and generalization in deep reinforcement learning. In International Conference on Machine Learning, pp. 14560\u201314581. PMLR, 2022.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan Ghasemzadeh. Lin- ear mode connectivity in multitask and continual learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Fmg_fQYUejf.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=0DcZxeWfOPt.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\nMartin Mundt, Steven Lang, Quentin Delfosse, and Kristian Kersting. CLEVA-compass: A continual learn- ing evaluation assessment compass to promote research transparency and comparability. International Conference on Learning Representations, 2022.\n15\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Computationally constrained reinforcement learning\n- Retrospective adversarial replay\n- Clever Hans predictors\n- Temporal generalization in neural language models\n- Offline reinforcement learning\n- Continuous transition\n- Sample efficiency\n- Self-improving reactive agents\n- Learning dynamics and generalization in deep reinforcement learning\n- Linear mode connectivity in multitask and continual learning\n- Fast model editing at scale\n- Human-level control through deep reinforcement learning\n- Continual learning evaluation assessment compass\n- Small losses in performance\n- Required adjustments\n- Limited compared to initial training phase\n- Less pronounced forgetting\n- Parameter regularization mechanisms\n- Functional regularization\n- Distillation approaches\n- Adapters\n- Low rank updates\n- Prompting\n- Freezing\n- Non-uniform learning rates\n- Limited data\n- Optimization issues\n- Overfitting issues\n- Comparatively small changes\n- Promising initial results\n- Paradigm shift\n- Modular approaches\n- Different components\n- Stitched together\n- Representation learning\n- Contrastive unsupervised learning\n- Domain specific experts\n- Multiple modalities\n- End-to-end learning\n- Beyond end-to-end paradigms\n- Training dynamics\n- Smaller-scale models\n- New data\n- Pre-training data\n- Interplay between memory and learning\n- Exploiting the modular structure\n- Specific ways to address the continual learning problem."
    },
    "15": {
        "chunk": " Martin Mundt, Yongwon Hong, Iuliia Pliushch, and Visvanathan Ramesh. A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning. Neural Networks, 160:306\u2013336, 2023.\nV.L. Nguyen, M.H. Shaker, and E. Hu\u0308llermeier. How to measure uncertainty in uncertainty sampling for active learning. Machine Learning, 111(1):89\u2013122, 2022. doi: 10.1007/s10994-021-06003-9.\nMaxime Oquab, Timothe\u0301e Darcet, The\u0301o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\nOleksiy Ostapenko, Timothee Lesort, Pau Rodri\u0301guez, Md Rifat Arefin, Arthur Douillard, Irina Rish, and Laurent Charlin. Foundational models for continual learning: An empirical study of latent replay, 2022. URL https://arxiv.org/abs/2205.00329.\nAristeidis Panos, Yuriko Kobe, Daniel Olmeda Reino, Rahaf Aljundi, and Richard E Turner. First session adaptation: A strong replay-free baseline for class-incremental learning. arXiv preprint arXiv:2303.13199, 2023.\nAnastasia Pentina and Christoph H. Lampert. A PAC-bayesian bound for lifelong learning. In ICML, 2014.\nIuliia Pliushch, Martin Mundt, Nicolas Lupp, and Visvanathan Ramesh. When Deep Classifiers Agree: Analyzing Correlations Between Learning Order and Image Statistics. European Conference on Computer Vision (ECCV), pp. 397\u2013413, 2022.\nAmeya Prabhu, Hasan Abed Al Kader Hammoud, Puneet K Dokania, Philip HS Torr, Ser-Nam Lim, Bernard Ghanem, and Adel Bibi. Computationally budgeted continual learning: What does matter? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3698\u20133707, 2023a.\nAmeya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, and Ozan Sener. Online con- tinual learning without the storage constraint. arXiv preprint arXiv:2305.09253, 2023b.\nDiana Benavides Prado and Patricia Riddle. A theory for knowledge transfer in continual learning. In Conference on Lifelong Learning Agents, 2022.\nAlexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals, Demis Has- sabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In International Conference on Machine Learning, pp. 2827\u20132836. PMLR, 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.\nRahul Ramesh and Pratik Chaudhari. Model zoo: A growing\" brain\" that learns continually. arXiv preprint arXiv:2106.03027, 2021.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. Advances in Neural Information Processing Systems, 32, 2019.\nKhalid Salama, Jarek Kazmierczak, and Donna Schut. Practitioners guide to MLOPS: A framework for continuous delivery and automation of machine learning. Google Could White paper, 2021.\nShibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry. Editing a classifier by rewriting its prediction rules. Advances in Neural Information Processing Systems, 34:23359\u201323373, 2021.\nTom Schaul, John Quan andIoannis Antonoglou, and David Silver. Prioritized experience replay. In 4th International Conference on Learning Representations, ICLR 2016, 2016.\n16\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Deep neural networks\n- Wholistic view\n- Forgotten lessons\n- Active and open world learning\n- Uncertainty sampling\n- Measure uncertainty\n- Active learning\n- Robust visual features\n- Foundational models\n- Latent replay\n- Class-incremental learning\n- PAC-Bayesian bound\n- Learning order and image statistics\n- Computationally budgeted continual learning\n- Online continual learning\n- Storage constraint\n- Knowledge transfer\n- Theory for knowledge transfer\n- Neural episodic control\n- Transferable visual models\n- Model zoo\n- Growing brain\n- Experience replay\n- Continuous delivery and automation\n- Machine learning operations (MLOPS)\n- Framework\n- Small losses in performance\n- Required adjustments\n- Limited compared to initial training phase\n- Less pronounced forgetting\n- Parameter regularization mechanisms\n- Functional regularization\n- Distillation approaches\n- Adapters\n- Low rank updates\n- Prompting\n- Freezing\n- Non-uniform learning rates\n- Limited data\n- Optimization issues\n- Overfitting issues\n- Comparatively small changes\n- Promising initial results\n- Paradigm shift\n- Modular approaches\n- Different components\n- Stitched together\n- Representation learning\n- Contrastive unsupervised learning\n- Domain specific experts\n- Multiple modalities\n- End-to-end learning\n- Beyond end-to-end paradigms\n- Training dynamics\n- Smaller-scale models\n- New data\n- Pre-training data\n- Interplay between memory and learning\n- Exploiting the modular structure\n- Specific ways to address the continual learning problem."
    },
    "16": {
        "chunk": " Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green AI. Communications of the ACM, 63 (12):54\u201363, 2020.\nBurr Settles. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin\u2013Madison, 2009.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.html.\nTinne Tuytelaars, Bing Liu, Vincenzo Lomonaco, Gido van de Ven, and Andrea Cossu. Deep Continual Learning (Dagstuhl Seminar 23122). Dagstuhl Reports, 13(3):74\u201391, 2023. ISSN 2192-5283. doi: 10.4230/ DagRep.13.3.74. URL https://drops.dagstuhl.de/entities/document/10.4230/DagRep.13.3.74.\nEuropean Union. Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation). Official Journal L110, 59:1\u201388, 2016.\nGido M van de Ven, Tinne Tuytelaars, and Andreas S Tolias. Three types of incremental learning. Nature Machine Intelligence, 4(12):1185\u20131197, 2022.\nLiyuan Wang, Xingxing Zhang, Qian Li, Jun Zhu, and Yi Zhong. Coscl: Cooperation of small continual learners is stronger than a big one. In European Conference on Computer Vision, pp. 254\u2013271. Springer, 2022a.\nZifeng Wang, Zheng Zhan, Yifan Gong, Geng Yuan, Wei Niu, Tong Jian, Bin Ren, Stratis Ioannidis, Yanzhi Wang, and Jennifer Dy. SparCL: Sparse continual learning on the edge. Advances in Neural Information Processing Systems, 35:20366\u201320380, 2022b.\nZifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 139\u2013149, 2022c.\nMaciej Wo\u0142czyk, Micha\u0142 Zaja\u0328c, Razvan Pascanu, \u0141ukasz Kucin\u0301ski, and Piotr Mi\u0142os\u0301. Continual world: A robotic benchmark for continual reinforcement learning. Advances in Neural Information Processing Sys- tems, 34:28496\u201328510, 2021.\nMaciej Wolczyk, Micha\u0142 Zaja\u0328c, Razvan Pascanu, \u0141ukasz Kucin\u0301ski, and Piotr Mi\u0142os\u0301. Disentangling transfer in continual reinforcement learning. Advances in Neural Information Processing Systems, 35:6304\u20136317, 2022.\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pp. 7959\u20137971, 2022.\nJiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language mod- els meet world models: Embodied experiences enhance language models. arXiv preprint arXiv:2305.10626, 2023.\nShipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremen- tal learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3014\u20133023, 2021.\n17\n",
        "summary": "arXiv:2305.12345v1 [cs.LR] 1 Jun 2023\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Atari\n- Go\n- Chess\n- Shogi\n- Green AI\n- Active learning\n- Reinforcement learning\n- Deep continual learning\n- Dagstuhl seminar\n- General data protection regulation\n- Incremental learning\n- Types of incremental learning\n- Cooperation of small continual learners\n- Sparse continual learning\n- Learning to prompt for continual learning\n- Continual reinforcement learning\n- Robotic benchmark\n- Disentangling transfer in continual reinforcement learning\n- Robust fine-tuning of zero-shot models\n- Language models\n- Embodied experiences\n- Dynamically expandable representation\n- Class incremental learning\n- Small losses in performance\n- Required adjustments\n- Limited compared to initial training phase\n- Less pronounced forgetting\n- Parameter regularization mechanisms\n- Functional regularization\n- Distillation approaches\n- Adapters\n- Low rank updates\n- Prompting\n- Freezing\n- Non-uniform learning rates\n- Limited data\n- Optimization issues\n- Overfitting issues\n- Comparatively small changes\n- Promising initial results\n- Paradigm shift\n- Modular approaches\n- Different components\n- Stitched together\n- Representation learning\n- Contrastive unsupervised learning\n- Domain specific experts\n- Multiple modalities\n- End-to-end learning\n- Beyond end-to-end paradigms\n- Training dynamics\n- Smaller-scale models\n- New data\n- Pre-training data\n- Interplay between memory and learning\n- Exploiting the modular structure\n- Specific ways to address the continual learning problem."
    },
    "17": {
        "chunk": " Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 3712\u20133722, 2018.\nZeyu Zhao. The application of the right to be forgotten in the machine learning context: From the perspective of european laws. Cath. UJL & Tech, 31:73, 2022.\nAlexander Zimin and Christoph H. Lampert. Tasks without borders: A new approach to online multi-task learning. In ICML Workshop on Adaptive & Multitask Learning, 2019. URL https://openreview.net/ forum?id=HkllV5Bs24.\nA Survey details\nTo verify the keywords \u2018incremental\u2019, \u2018continual\u2019, \u2018forgetting\u2019, \u2018lifelong\u2019 and \u2018catastrophic\u2019, used to filter the papers based on their titles, we tested them using a manually collected validation set of which we are certain that they are continual learning related. This set was manually collected while doing research on continual learning over the past few years. The keywords were present in 96% of the paper titles. From each conference, we randomly picked 20 out of all matched papers, disregarding false positives.\nIt is common for to evaluate new methods and analyses on more than one benchmark. Often this means that the percentage of stored samples is not uniform across the experiments in a paper. In Figure 1, we showed the minimum percentage used, in Figure 3 we show the maximum. The conclusion remains the same, and the amount of stored samples is constrained in all but two benchmarks.\nIn Table 1 we provide a table of all the papers we used in the survey of Section 2, showing their minimal and maximal sample store ratio (SSR) i.e. the percentage of samples stored, as well as possibly other memory consumption. The last column mentions how they approached the computational cost.\nOptimized\nConstrained\nCompared\nDiscussed\nNot discussed\n34\nNone \u22641% 1-5% 5-10% 10-99% 100%\nPercentage of stored data\nFigure 3: Most papers strongly restrict memory use and do not discuss computational cost. This figure is an alternate version of Figure 1, with the maximum percentage of stored samples rather than the minimum. Each dot represents one paper, illustrating what percentage of data their methods store (horizontal axis) and how computational complexity is handled (vertical axis). The majority of surveyed papers are in the lower-left corner: those that strongly restrict memory use and do not quantitatively approach computational cost (i.e. it is at most discussed). For more details, see Appendix.\n18\n 1\n3\n10\n12\n      Computational cost\n",
        "summary": "Limited\n  Discussed\n     Quantitatively\n   Comparison\n    Optimized\n\n Central Concepts, Theories, or Ideas:\n\n- Continual learning (or lifelong learning or incremental learning)\n- Taskonomy\n- Right to be forgotten\n- European laws\n- Validation set\n- Keywords\n- Benchmark\n- Percentage of stored samples\n- Memory consumption\n- False positives\n- Minimal and maximal sample store ratio (SSR)\n- Computational cost\n- Optimization\n- Constrained\n- Compared\n- Discussed\n- Not discussed\n- Small losses in performance\n- Required adjustments\n- Limited compared to initial training phase\n- Less pronounced forgetting\n- Parameter regularization mechanisms\n- Functional regularization\n- Distillation approaches\n- Adapters\n- Low rank updates\n- Prompting\n- Freezing\n- Non-uniform learning rates\n- Limited data\n- Optimization issues\n- Overfitting issues\n- Comparatively small changes\n- Importance of continual learning\n- Survey\n- Smaller-scale models\n- New data\n- Pre-training data\n- Interplay between memory and learning\n- Exploiting the modular structure\n- Specific ways to address the continual learning problem."
    }
}